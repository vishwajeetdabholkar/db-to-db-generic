{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6921b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5274f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.app.name', 'test'),\n",
       " ('spark.driver.host', 'PF28CEFN.mshome.net'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.id', 'local-1669980639542'),\n",
       " ('spark.app.submitTime', '1669980637109'),\n",
       " ('spark.driver.port', '50369'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.startTime', '1669980637293'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "spark.sparkContext._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fcec198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark.sql.functions as F\n",
    "from logging import Logger\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "277f9800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa62f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_config = {\n",
    "        \"url\" : \"jdbc:postgresql://localhost:5432/dbt-workshop\",\n",
    "        \"driver\" : \"org.postgresql.Driver\",\n",
    "        \"user\" : \"postgres\",\n",
    "        \"password\" : \"admin\"\n",
    "    }\n",
    "\n",
    "target_config = {\n",
    "        \"url\" : \"jdbc:postgresql://localhost:5432/dbt-workshop\",\n",
    "        \"driver\" : \"org.postgresql.Driver\",\n",
    "        \"user\" : \"postgres\",\n",
    "        \"password\" : \"admin\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194071de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_pg(config: dict, sql: str, table: str,spark) -> DataFrame:\n",
    "    \"\"\" Read dataframe from postgres\n",
    "    Args:\n",
    "        config: settings for connect\n",
    "        sql: sql to read, it may be one of these format\n",
    "             - 'table_name'\n",
    "             - 'schema_name.table_name'\n",
    "             - '(select a, b, c from t1 join t2 ...) as foo'\n",
    "        spark: specific current spark_context or None\n",
    "        logger: logger\n",
    "    Returns:\n",
    "        selected DF\n",
    "    \"\"\"\n",
    "    sql = ''\n",
    "    # if logger:\n",
    "    #     logger.info('read_from_pg:\\n{}'.format(sql))\n",
    "        \n",
    "    if sql:\n",
    "        source_df = spark.read.format(\"jdbc\").options(**config).option(\"query\", sql).load()\n",
    "        \n",
    "    else:\n",
    "        source_df = spark.read.format(\"jdbc\").options(**config).option('dbtable',table).load()\n",
    "        \n",
    "    return source_df \n",
    "\n",
    "\n",
    "\n",
    "def write_to_pg(df: DataFrame, config: dict, table: str, column_list: list, mode: str='append' ) -> None:\n",
    "    \"\"\" Write dataframe to postgres\n",
    "    Args:\n",
    "        df: DataFrame to write\n",
    "        config: config dict\n",
    "        table: table_name in which we write_data\n",
    "        column_list: list of columns in which we write the data\n",
    "        logger: logger\n",
    "        mode: mode, one of these:\n",
    "            - append - create table if not exists (with all columns of DataFrame)\n",
    "                and write records to table (using fields only in table columns)\n",
    "            - overwrite - truncate table (if exists) and write records (using fields only in table columns)\n",
    "            - overwrite_full - drop table and create new one with all columns and DataFrame and append records to it\n",
    "            - fail - fail if table is not exists, otherwise append records to it\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # df.select(*column_list).write.format('jdbc').options(**config).option('dbtable',table).mode(mode).save()\n",
    "\n",
    "        df.select(*column_list).write.format('jdbc').options(\n",
    "                                        url = \"jdbc:postgresql://localhost:5432/dbt-workshop\",\n",
    "                                        driver = \"org.postgresql.Driver\",\n",
    "                                        user = \"postgres\",\n",
    "                                        password = \"admin\"\n",
    "                                        ).option('dbtable',table).mode(mode).save()\n",
    "\n",
    "        return \"data written into postgresql successfully\"\n",
    "    except Exception as e:\n",
    "        return f\"{e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8a826d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_CONFIG = {\n",
    "    \"MASTER\": \"local[*]\",\n",
    "    \"settings\": {\n",
    "      \"spark.executor.cores\": \"1\",\n",
    "      \"spark.executor.memory\": \"1g\",\n",
    "      \"spark.driver.cores\": \"1\",\n",
    "      \"spark.driver.memory\": \"1g\",\n",
    "      \"spark.cores.max\": \"1\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def init_spark_context(appname: str, jar_path:str) -> SparkContext:\n",
    "    \"\"\" init spark context \"\"\"\n",
    "\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = f'--jars {jar_path} pyspark-shell'\n",
    "    conf = SparkConf()\n",
    "    conf.setMaster(SPARK_CONFIG['MASTER'])\n",
    "    conf.setAppName(appname)\n",
    "\n",
    "    for setting, value in SPARK_CONFIG['settings'].items():\n",
    "        conf.set(setting, value)\n",
    "\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "    return sc\n",
    "\n",
    "jar_path = 'jars/postgresql-42.5.0.jar'\n",
    "sc = init_spark_context('app', jar_path)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7302c6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- orderid: integer (nullable = true)\n",
      " |-- paymentmethod: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- created: date (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "table = \"dev.stripe_payments\"\n",
    "df = read_from_pg(source_config, '' , table, spark)\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec298d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3da2aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data written into postgresql successfully'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_to_pg(df, target_config, 'dev.test_table', columns_list, 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1c06d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import Logger\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "104dcb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_pg(spark:SparkSession, df: DataFrame, config: dict, table: str, mode: str='append' ) -> None:\n",
    "    \"\"\" Write dataframe to postgres\n",
    "    Args:\n",
    "        df: DataFrame to write\n",
    "        config: config dict\n",
    "        table: table_name in which we write_data\n",
    "        column_list: list of columns in which we write the data\n",
    "        mode: mode, one of these:\n",
    "            - append - create table if not exists (with all columns of DataFrame)\n",
    "                and write records to table (using fields only in table columns)\n",
    "            - overwrite - truncate table (if exists) and write records (using fields only in table columns)\n",
    "            - overwrite_full - drop table and create new one with all columns and DataFrame and append records to it\n",
    "            - fail - fail if table is not exists, otherwise append records to it\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        column_list = df.columns\n",
    "        if len(column_list) == 0:\n",
    "            return(\"No columns to write into\")\n",
    "\n",
    "        else:\n",
    "            df.select(*column_list).write.format('jdbc').options(**config).option('dbtable',table).mode(mode).save()\n",
    "            return \"Data written into postgresql successfully\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        return f\"{e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "547727a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "            \"url\": \"jdbc:postgresql://localhost:5432/postgres\",\n",
    "            \"driver\": \"org.postgresql.Driver\",\n",
    "            \"user\": \"postgres\",\n",
    "            \"password\": \"admin\"\n",
    "}\n",
    "table = 'emp_details'\n",
    "mode = 'append'\n",
    "sql = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6038d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('header',True).csv('source_data.csv', inferSchema =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "418f1c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d0b88d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d1f0f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_to_pg(spark, df, config, table, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8a3e7dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_pg(spark:SparkSession, config: dict, sql: str, table: str) -> DataFrame:\n",
    "    \"\"\" Read dataframe from postgres\n",
    "    Args:\n",
    "        config: settings for connect\n",
    "        sql: sql to read, it may be one of these format\n",
    "             - 'table_name'\n",
    "             - 'schema_name.table_name'\n",
    "             - '(select a, b, c from t1 join t2 ...) as foo'\n",
    "        spark: specific current spark_context or None\n",
    "    Returns:\n",
    "        selected DF\n",
    "    \"\"\"        \n",
    "    print(\"reading from postgresql\")\n",
    "    if sql:\n",
    "        print(\"executing query to create df\")\n",
    "        source_df = spark.read.format(\"jdbc\").options(**config).option(\"query\", sql).load()\n",
    "        \n",
    "    else:\n",
    "        print(\"reading directly from source table\")\n",
    "        source_df = spark.read.format(\"jdbc\").options(**config).option('dbtable',table).load()\n",
    "        \n",
    "    return source_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a24e5736",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_config = {\n",
    "            \"url\": \"jdbc:postgresql://localhost:5432/postgres\",\n",
    "            \"driver\": \"org.postgresql.Driver\",\n",
    "            \"user\": \"postgres\",\n",
    "            \"password\": \"admin\"\n",
    "}\n",
    "source_table = 'source_emp_details'\n",
    "sql = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73b71e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from postgresql\n",
      "reading directly from source table\n"
     ]
    }
   ],
   "source": [
    "source_df = read_from_pg(spark, source_config, sql, source_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11dbcf01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('fname', StringType(), True), StructField('lname', StringType(), True), StructField('age', StringType(), True), StructField('dob', StringType(), True), StructField('salary', StringType(), True)])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d91b205c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9cafb052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---+----------+-------+\n",
      "| fname|    lname|age|       dob| salary|\n",
      "+------+---------+---+----------+-------+\n",
      "|  vish|dabholkar| 25|21-02-1998|   1000|\n",
      "|akshay|dabholkar| 28|25-03-1995|2500.56|\n",
      "|   sam|    patil| 24|27-09-1999| 524.44|\n",
      "+------+---------+---+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1e82d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_config = {\n",
    "            \"url\": \"jdbc:postgresql://localhost:5432/postgres\",\n",
    "            \"driver\": \"org.postgresql.Driver\",\n",
    "            \"user\": \"postgres\",\n",
    "            \"password\": \"admin\"\n",
    "}\n",
    "target_table = 'target_emp_details'\n",
    "sql = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b88e621f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from postgresql\n",
      "reading directly from source table\n"
     ]
    }
   ],
   "source": [
    "target_df = read_from_pg(spark, target_config, sql, target_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1ba61c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- dob: date (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40437ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.StructType"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72d35ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "20c6104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mappings(spark:SparkSession):\n",
    "    mapping_file_path = 'mapping.csv'\n",
    "    mapping_df = spark.read.option('header',True).csv(mapping_file_path)\n",
    "    source_column_list = list(mapping_df.select('source').toPandas()['source'])\n",
    "    target_column_list = list(mapping_df.select('target').toPandas()['target'])\n",
    "    source_target_dict = dict(zip(source_column_list, target_column_list))\n",
    "    return source_target_dict\n",
    "\n",
    "mapping = get_mappings(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb967b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2506f7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from postgresql\n",
      "reading directly from source table\n"
     ]
    }
   ],
   "source": [
    "source_df = read_from_pg(spark, source_config, sql, source_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "40272899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "459d88bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[fname: string, lname: string, age: string, dob: string, salary: string]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df.select([col(c).cast(\"string\") for c in target_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "68242d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- dob: date (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"id\", F.col(\"new_id\").cast(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "12ea720c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fname', 'string'),\n",
       " ('lname', 'string'),\n",
       " ('age', 'int'),\n",
       " ('dob', 'date'),\n",
       " ('salary', 'double')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "10f22d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = source_df.withColumn(\"salary\",source_df.salary.cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a664c5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf9a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cx_Oracle\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "914e6324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pg_table_metadeta(config:dict, table_name):\n",
    "    \"\"\"\n",
    "    Function to get table metadeta from a postgresql table\n",
    "    Args:\n",
    "        config: It is a dict with parameters required to connect to a postgres sever\n",
    "            like hostname, db name, username and password\n",
    "            \n",
    "        table_name: name of the table for which we want to get the metadata\n",
    "        \n",
    "    return: \n",
    "        pandas dataframe for postgresql table metadeta\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        host = config['host']\n",
    "        database = config['database']\n",
    "        user = config['user']\n",
    "        password = config['password']\n",
    "\n",
    "        conn = psycopg2.connect(\n",
    "                    host=host,\n",
    "                    database=database,\n",
    "                    user=user,\n",
    "                    password=password)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        query = f''' select column_name, data_type, character_maximum_length, column_default, is_nullable\n",
    "    from INFORMATION_SCHEMA.COLUMNS where table_name = '{table_name}'; '''\n",
    "        df = pd.read_sql_query(query ,con=conn)\n",
    "        conn.close()\n",
    "        return df\n",
    "    \n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "            print('Database connection closed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c2a51bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oracle_table_metadeta(config:dict, table_name):\n",
    "    \"\"\"\n",
    "    Function to get table metadeta from a oracle db table\n",
    "    Args:\n",
    "        config: It is a dict with parameters required to connect to a oracle db sever\n",
    "            like dsn, username and password\n",
    "            \n",
    "        table_name: name of the table for which we want to get the metadata\n",
    "        \n",
    "    return: \n",
    "        pandas dataframe for postgresql table metadeta\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dsn = config['dsn']\n",
    "        username = config['user']\n",
    "        password = config['password']\n",
    "        \n",
    "        conn = cx_Oracle.connect(user=username, \n",
    "                                 password=password,\n",
    "                               dsn=dsn,\n",
    "                               encoding=\"UTF-8\")\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        query = f'''  select \n",
    "            t.table_name as table_name  \n",
    "              , t.column_name                        \n",
    "              , t.data_type\n",
    "              , cc.constraint_name\n",
    "              , uc.constraint_type\n",
    "           from user_tab_columns t\n",
    "                left join user_cons_columns cc\n",
    "                  on (cc.table_name = t.table_name and\n",
    "                      cc.column_name = t.column_name)\n",
    "                left join user_constraints uc\n",
    "                  on (t.table_name = uc.table_name and\n",
    "                      uc.constraint_name = cc.constraint_name )\n",
    "         where t.table_name in ('{table_name}')'''\n",
    "\n",
    "        df = pd.read_sql_query(query ,con=conn)\n",
    "        return df\n",
    "    \n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "            print('Database connection closed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60bfb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_sever_table_metadeta(config:dict, table_name):\n",
    "    \"\"\"\n",
    "    Function to get table metadeta from a sql_sever db table\n",
    "    Args:\n",
    "        config: It is a dict with parameters required to connect to a sql_sever \n",
    "            like host, database, username and password\n",
    "            \n",
    "        table_name: name of the table for which we want to get the metadata\n",
    "        \n",
    "    return: \n",
    "        pandas dataframe for postgresql table metadeta\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = pyodbc.connect(\n",
    "                \"Driver={ODBC Driver 18 for SQL Server};\"\n",
    "                        f\"Server={host};\"\n",
    "                        f\"Database={database};\"\n",
    "                        f\"uid={user};pwd={password}\")\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        query = f'''  '{table_name}' '''\n",
    "\n",
    "        df = pd.read_sql_query(query ,con=conn)\n",
    "        return df\n",
    "    \n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "            print('Database connection closed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91012e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecabab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"select * from INFORMATION_SCHEMA.COLUMNS where table_name = {table_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d844d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "240bd980",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'cmx_ors'\n",
    "password = 'cmx_ors'\n",
    "dsn = '192.168.2.70:1521/PDB'\n",
    "table_name = 'C_BO_ADDR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "66cff806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  select \n",
      "    t.table_name as table_name  \n",
      "      , t.column_name                        \n",
      "      , t.data_type\n",
      "      , cc.constraint_name\n",
      "      , uc.constraint_type\n",
      "   from user_tab_columns t\n",
      "        left join user_cons_columns cc\n",
      "          on (cc.table_name = t.table_name and\n",
      "              cc.column_name = t.column_name)\n",
      "        left join user_constraints uc\n",
      "          on (t.table_name = uc.table_name and\n",
      "              uc.constraint_name = cc.constraint_name )\n",
      " where t.table_name in ('C_BO_ADDR')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishwajeet.dabholkar\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    conn = cx_Oracle.connect(user=username, \n",
    "                             password=password,\n",
    "                           dsn=dsn,\n",
    "                           encoding=\"UTF-8\")\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    query = f'''  select \n",
    "    t.table_name as table_name  \n",
    "      , t.column_name                        \n",
    "      , t.data_type\n",
    "      , cc.constraint_name\n",
    "      , uc.constraint_type\n",
    "   from user_tab_columns t\n",
    "        left join user_cons_columns cc\n",
    "          on (cc.table_name = t.table_name and\n",
    "              cc.column_name = t.column_name)\n",
    "        left join user_constraints uc\n",
    "          on (t.table_name = uc.table_name and\n",
    "              uc.constraint_name = cc.constraint_name )\n",
    " where t.table_name in ('{table_name}')'''\n",
    "    print(query)\n",
    "    df = pd.read_sql_query(query ,con=conn)\n",
    "except (Exception, psycopg2.DatabaseError) as error:\n",
    "    print(error)\n",
    "finally:\n",
    "    if conn is not None:\n",
    "        conn.close()\n",
    "        print('Database connection closed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e267ec97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ea468ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TABLE_NAME         23\n",
       "COLUMN_NAME        23\n",
       "DATA_TYPE          23\n",
       "CONSTRAINT_NAME     5\n",
       "CONSTRAINT_TYPE     5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b4246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0847a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'localhost'\n",
    "database = 'postgres'\n",
    "user = 'postgres'\n",
    "password = 'admin'\n",
    "table_name = 'target_emp_details'\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "                host=host,\n",
    "                database=database,\n",
    "                user=user,\n",
    "                password=password)\n",
    "cur = conn.cursor()\n",
    "\n",
    "\n",
    "query = f''' select column_name, data_type, character_maximum_length, column_default, is_nullable\n",
    "from INFORMATION_SCHEMA.COLUMNS where table_name = '{table_name}'; '''\n",
    "query\n",
    "\n",
    "\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a487dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d3256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed6b7976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b9c977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d7eff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3563354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70bcf51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishwajeet.dabholkar\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_sql_query(query ,con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09eb27c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>character_maximum_length</th>\n",
       "      <th>column_default</th>\n",
       "      <th>is_nullable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>integer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dob</td>\n",
       "      <td>date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>salary</td>\n",
       "      <td>double precision</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fname</td>\n",
       "      <td>character varying</td>\n",
       "      <td>50.0</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lname</td>\n",
       "      <td>character varying</td>\n",
       "      <td>50.0</td>\n",
       "      <td>None</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  column_name          data_type  character_maximum_length column_default  \\\n",
       "0         age            integer                       NaN           None   \n",
       "1         dob               date                       NaN           None   \n",
       "2      salary   double precision                       NaN           None   \n",
       "3       fname  character varying                      50.0           None   \n",
       "4       lname  character varying                      50.0           None   \n",
       "\n",
       "  is_nullable  \n",
       "0         YES  \n",
       "1         YES  \n",
       "2         YES  \n",
       "3         YES  \n",
       "4         YES  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "251ee455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishwajeet.dabholkar\\AppData\\Local\\Temp\\ipykernel_12948\\496160328.py:1: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
      "  df.style.hide_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_00b5b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_00b5b_level0_col0\" class=\"col_heading level0 col0\" >column_name</th>\n",
       "      <th id=\"T_00b5b_level0_col1\" class=\"col_heading level0 col1\" >data_type</th>\n",
       "      <th id=\"T_00b5b_level0_col2\" class=\"col_heading level0 col2\" >character_maximum_length</th>\n",
       "      <th id=\"T_00b5b_level0_col3\" class=\"col_heading level0 col3\" >column_default</th>\n",
       "      <th id=\"T_00b5b_level0_col4\" class=\"col_heading level0 col4\" >is_nullable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_00b5b_row0_col0\" class=\"data row0 col0\" >age</td>\n",
       "      <td id=\"T_00b5b_row0_col1\" class=\"data row0 col1\" >integer</td>\n",
       "      <td id=\"T_00b5b_row0_col2\" class=\"data row0 col2\" >nan</td>\n",
       "      <td id=\"T_00b5b_row0_col3\" class=\"data row0 col3\" >None</td>\n",
       "      <td id=\"T_00b5b_row0_col4\" class=\"data row0 col4\" >YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00b5b_row1_col0\" class=\"data row1 col0\" >dob</td>\n",
       "      <td id=\"T_00b5b_row1_col1\" class=\"data row1 col1\" >date</td>\n",
       "      <td id=\"T_00b5b_row1_col2\" class=\"data row1 col2\" >nan</td>\n",
       "      <td id=\"T_00b5b_row1_col3\" class=\"data row1 col3\" >None</td>\n",
       "      <td id=\"T_00b5b_row1_col4\" class=\"data row1 col4\" >YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00b5b_row2_col0\" class=\"data row2 col0\" >salary</td>\n",
       "      <td id=\"T_00b5b_row2_col1\" class=\"data row2 col1\" >double precision</td>\n",
       "      <td id=\"T_00b5b_row2_col2\" class=\"data row2 col2\" >nan</td>\n",
       "      <td id=\"T_00b5b_row2_col3\" class=\"data row2 col3\" >None</td>\n",
       "      <td id=\"T_00b5b_row2_col4\" class=\"data row2 col4\" >YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00b5b_row3_col0\" class=\"data row3 col0\" >fname</td>\n",
       "      <td id=\"T_00b5b_row3_col1\" class=\"data row3 col1\" >character varying</td>\n",
       "      <td id=\"T_00b5b_row3_col2\" class=\"data row3 col2\" >50.000000</td>\n",
       "      <td id=\"T_00b5b_row3_col3\" class=\"data row3 col3\" >None</td>\n",
       "      <td id=\"T_00b5b_row3_col4\" class=\"data row3 col4\" >YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00b5b_row4_col0\" class=\"data row4 col0\" >lname</td>\n",
       "      <td id=\"T_00b5b_row4_col1\" class=\"data row4 col1\" >character varying</td>\n",
       "      <td id=\"T_00b5b_row4_col2\" class=\"data row4 col2\" >50.000000</td>\n",
       "      <td id=\"T_00b5b_row4_col3\" class=\"data row4 col3\" >None</td>\n",
       "      <td id=\"T_00b5b_row4_col4\" class=\"data row4 col4\" >YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1fc28a2c6a0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31d467f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ada0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c64ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa60ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea03e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d67b98e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90dac0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef32d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a748ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f44c01c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual= ['CREATE_DATE','DELETED_DATE','CITY','CNTRY','STATE','PIN_CD','DIRTY_IND', 'dob']\n",
    "predicted = ['created_date','deleted_date','city','country','state','pincode','dirty_ind','source_name', 'date_of_birth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9063cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import csv\n",
    "\n",
    "threshold = 80\n",
    "result = {}\n",
    "for act in actual:\n",
    "    for pre in predicted:\n",
    "        # print(act,':',pre)\n",
    "        if pre in actual:\n",
    "            result[act] : pre\n",
    "        \n",
    "        elif fuzz.token_set_ratio(act.lower(),pre.lower()) >= threshold:            \n",
    "            result[act] = pre\n",
    "            \n",
    "        else:\n",
    "            result[''] = pre\n",
    "            \n",
    "            \n",
    "            \n",
    "source = list(result.keys())\n",
    "target = list(result.values())\n",
    "\n",
    "with open(r'mapping_tg.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['source','source_data_type','source_format','target','target_data_type','target_format','default_value']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for key, values in result.items():\n",
    "        writer.writerow({'source':key, 'target':values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "461a50da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CREATE_DATE': 'created_date',\n",
       " '': 'date_of_birth',\n",
       " 'DELETED_DATE': 'deleted_date',\n",
       " 'CITY': 'city',\n",
       " 'CNTRY': 'country',\n",
       " 'STATE': 'state',\n",
       " 'DIRTY_IND': 'dirty_ind'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ad10225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import csv\n",
    "\n",
    "def generate_mapping_file(actual, predicted):\n",
    "    \"\"\"\n",
    "    Function to generate mapping file which will have source to target column mapping\n",
    "    this mapping is atuo generated based on the column names.\n",
    "    fucntion writes mapping into a file which should be validated by the user before using it for data migration\n",
    "    Args:\n",
    "        actual: source column name list\n",
    "        predicted: target column list\n",
    "    \"\"\"\n",
    "    try:        \n",
    "        threshold = 80\n",
    "        result = {}\n",
    "        for act in actual:\n",
    "            for pre in predicted:\n",
    "                # print(act,':',pre)\n",
    "                if pre in actual:\n",
    "                    result[act] : pre\n",
    "\n",
    "                elif fuzz.token_set_ratio(act.lower(),pre.lower()) >= threshold:            \n",
    "                    result[act] = pre\n",
    "\n",
    "                else:\n",
    "                    result[''] = pre\n",
    "\n",
    "        source = list(result.keys())\n",
    "        target = list(result.values())\n",
    "\n",
    "        with open(r'mapping_tg.csv', 'w', newline='') as csvfile:\n",
    "            fieldnames = ['source','source_data_type','source_format','target','target_data_type','target_format','default_value']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for key, values in result.items():\n",
    "                writer.writerow({'source':key, 'target':values})\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        return f\"{e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "00aec715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3e5d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce88977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
