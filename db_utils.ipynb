{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14194701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import Logger\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, unix_timestamp, to_date, date_format, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a67650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_pg(spark:SparkSession, config: dict, sql: str, table: str) -> DataFrame:\n",
    "    \"\"\" Read dataframe from postgres\n",
    "    Args:\n",
    "        config: settings for connect\n",
    "        sql: sql to read, it may be one of these format\n",
    "             - 'table_name'\n",
    "             - 'schema_name.table_name'\n",
    "             - '(select a, b, c from t1 join t2 ...) as foo'\n",
    "        spark: specific current spark_context or None\n",
    "    Returns:\n",
    "        selected DF\n",
    "    \"\"\"        \n",
    "    try:\n",
    "        print(\"reading from postgresql\")\n",
    "        if sql:\n",
    "            print(\"executing query to create df\")\n",
    "            source_df = spark.read.format(\"jdbc\").options(**config).option(\"query\", sql).load()\n",
    "\n",
    "        else:\n",
    "            print(\"reading directly from source table\")\n",
    "            source_df = spark.read.format(\"jdbc\").options(**config).option('dbtable',table).load()\n",
    "\n",
    "        return source_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        return f\"{e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f468c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_pg(spark:SparkSession, df: DataFrame, config: dict, table: str, mode: str='append' ) -> None:\n",
    "    \"\"\" Write dataframe to postgres\n",
    "    Args:\n",
    "        df: DataFrame to write\n",
    "        config: config dict\n",
    "        table: table_name in which we write_data\n",
    "        column_list: list of columns in which we write the data\n",
    "        mode: mode, one of these:\n",
    "            - append - create table if not exists (with all columns of DataFrame)\n",
    "                and write records to table (using fields only in table columns)\n",
    "            - overwrite - truncate table (if exists) and write records (using fields only in table columns)\n",
    "            - overwrite_full - drop table and create new one with all columns and DataFrame and append records to it\n",
    "            - fail - fail if table is not exists, otherwise append records to it\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        column_list = df.columns\n",
    "        if len(column_list) == 0:\n",
    "            return(\"No columns to write into\")\n",
    "\n",
    "        else:\n",
    "            df.select(*column_list).write.format('jdbc').options(**config).option('dbtable',table).mode(mode).save()\n",
    "            return \"Data written into postgresql successfully\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        return f\"{e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca9a588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_file(spark:SparkSession, filename:str, delimiter:str) -> DataFrame:\n",
    "    \"\"\" Read dataframe from manual file\n",
    "    Args:\n",
    "        filename: file to read from\n",
    "        delimiter: delimiter to use when reading the file\n",
    "    Returns:\n",
    "        selected DF\n",
    "    \"\"\"       \n",
    "    try:\n",
    "        print(\"reading from file\")\n",
    "        source_df = spark.read.option(\"delimiter\", delimiter).option('header',True).csv('sourcedata/'+filename)  \n",
    "        print(\"file read successfully, returning dataframe\")\n",
    "        return source_df \n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        return f\"{e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0065c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(df:DataFrame, filename:str, delimiter:str = ',')->None:\n",
    "    \"\"\"Write dataframe to a csv file\n",
    "    Args:\n",
    "    df: data frame to write\n",
    "    filename: filename with which we want to save our dataframe in a file\n",
    "    by default we will write a comma delimited file only \n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"writing the dataframe\")\n",
    "        df.coalesce(1).write.option('delimiter', delimiter).option(\"header\", True).csv('target/'+filename)\n",
    "        print(\"dataframe written at targets/\"+filename)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        return f\"{e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c128bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_oracle(spark:SparkSession, config: dict, sql: str, table: str) -> DataFrame:\n",
    "    \"\"\" Read dataframe from oracle\n",
    "    Args:\n",
    "        config: settings for connect\n",
    "        sql: sql to read, it may be one of these format\n",
    "             - 'table_name'\n",
    "             - 'schema_name.table_name'\n",
    "             - '(select a, b, c from t1 join t2 ...) as foo'\n",
    "        spark: specific current spark_context or None\n",
    "    Returns:\n",
    "        selected DF\n",
    "    \"\"\"        \n",
    "    try:\n",
    "        print(\"reading from oracle\")\n",
    "        if sql:\n",
    "            print(\"executing query to create df\")\n",
    "            source_df = spark.read.format(\"jdbc\").options(**config).option(\"query\", sql).load()\n",
    "\n",
    "        else:\n",
    "            print(\"reading directly from source table\")\n",
    "            source_df = spark.read.format(\"jdbc\").options(**config).option('dbtable',table).load()\n",
    "\n",
    "        return source_df \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        return f\"{e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77896517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_oracle(spark:SparkSession, df: DataFrame, config: dict, table: str, mode: str='append' ) -> None:\n",
    "    \"\"\" Write dataframe to oracle\n",
    "    Args:\n",
    "        df: DataFrame to write\n",
    "        config: config dict\n",
    "        table: table_name in which we write_data\n",
    "        column_list: list of columns in which we write the data\n",
    "        mode: mode, one of these:\n",
    "            - append - create table if not exists (with all columns of DataFrame)\n",
    "                and write records to table (using fields only in table columns)\n",
    "            - overwrite - truncate table (if exists) and write records (using fields only in table columns)\n",
    "            - overwrite_full - drop table and create new one with all columns and DataFrame and append records to it\n",
    "            - fail - fail if table is not exists, otherwise append records to it\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        column_list = df.columns\n",
    "        if len(column_list) == 0:\n",
    "            return(\"No columns to write into\")\n",
    "\n",
    "        else:\n",
    "            df.select(*column_list).write.format('jdbc').options(**config).option('dbtable',table).mode(mode).save()\n",
    "            return \"Data written into oracle successfully\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        return f\"{e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c637cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_msssql(spark:SparkSession, config: dict, sql: str, table: str) -> DataFrame:\n",
    "    \"\"\" Read dataframe from msssql\n",
    "    Args:\n",
    "        config: settings for connect\n",
    "        sql: sql to read, it may be one of these format\n",
    "             - 'table_name'\n",
    "             - 'schema_name.table_name'\n",
    "             - '(select a, b, c from t1 join t2 ...) as foo'\n",
    "        spark: specific current spark_context or None\n",
    "    Returns:\n",
    "        selected DF\n",
    "    \"\"\"       \n",
    "    try:\n",
    "        print(\"reading from msssql\")\n",
    "        if sql:\n",
    "            print(\"executing query to create df\")\n",
    "            source_df = spark.read.format(\"jdbc\").options(**config).option(\"query\", sql).load()\n",
    "\n",
    "        else:\n",
    "            print(\"reading directly from source table\")\n",
    "            source_df = spark.read.format(\"jdbc\").options(**config).option('dbtable',table).load()\n",
    "\n",
    "        return source_df \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        return f\"{e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aece80c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_mssql(spark:SparkSession, df: DataFrame, config: dict, table: str, mode: str='append' ) -> None:\n",
    "    \"\"\" Write dataframe to msssql\n",
    "    Args:\n",
    "        df: DataFrame to write\n",
    "        config: config dict\n",
    "        table: table_name in which we write_data\n",
    "        column_list: list of columns in which we write the data\n",
    "        mode: mode, one of these:\n",
    "            - append - create table if not exists (with all columns of DataFrame)\n",
    "                and write records to table (using fields only in table columns)\n",
    "            - overwrite - truncate table (if exists) and write records (using fields only in table columns)\n",
    "            - overwrite_full - drop table and create new one with all columns and DataFrame and append records to it\n",
    "            - fail - fail if table is not exists, otherwise append records to it\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        column_list = df.columns\n",
    "        if len(column_list) == 0:\n",
    "            return(\"No columns to write into\")\n",
    "\n",
    "        else:\n",
    "            df.select(*column_list).write.format('jdbc').options(**config).option('dbtable',table).mode(mode).save()\n",
    "            return \"Data written into msssql successfully\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        return f\"{e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92ba5b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_target_dtypes(source_df:DataFrame, target_df:DataFrame, mappings:dict, source_date_format: str, target_date_format: str)-> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to convert source column names to target column names \n",
    "    and convert source column data types to target column data types\n",
    "    \n",
    "    Args:\n",
    "        source_df: source dataframe \n",
    "        target_df: target dataframe\n",
    "        mapping: dictionary with source to column mapping \n",
    "        date_format: date format to which we want to convert source date column to target date type\n",
    "        \n",
    "    returns :\n",
    "        returns a data frame which we can write in target\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        print('source_types: ', source_df.dtypes)\n",
    "        target_types = target_df.dtypes\n",
    "        source_types = source_df.dtypes\n",
    "        print('target_types: ',target_types)\n",
    "        # date_format =  'dd-MM-yyyy'\n",
    "        \n",
    "        # mapping source columns to target\n",
    "        final_df = source_df\n",
    "        for key, value in mappings.items():\n",
    "            final_df= final_df.withColumnRenamed(key,value)\n",
    "            \n",
    "        \n",
    "        # type conversion of source columns to target column types\n",
    "\n",
    "        for i in source_types:\n",
    "            for j in target_types:\n",
    "                if i[0] == j[0] and j[1]!='date' :\n",
    "                    print(i[0])\n",
    "                    final_df = final_df.withColumn(i[0],final_df[i[0]].cast(j[1]))\n",
    "\n",
    "                elif i[0] == j[0] and j[1] =='date' :\n",
    "                    print(i[0])\n",
    "                    final_df = final_df.withColumn(i[0], to_timestamp(final_df[i[0]], source_date_format).cast('date') )\n",
    "                \n",
    "                elif i[0] == j[0] and j[1] =='timestamp' :\n",
    "                    print(i[0])\n",
    "                    final_df = final_df.withColumn(i[0], date_format(final_df[i[0]], target_date_format).cast('timestamp') )\n",
    "    \n",
    "        return final_df\n",
    "    except Exception as e:\n",
    "        print(\"Error occured\")\n",
    "        return f\"{e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca554684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b34f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
