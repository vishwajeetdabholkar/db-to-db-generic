{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14194701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import Logger\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, unix_timestamp, to_date, date_format, to_timestamp, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a67650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_pg(spark:SparkSession, config: dict, sql: str, table: str) -> DataFrame:\n",
    "    \"\"\" Read dataframe from postgres\n",
    "    Args:\n",
    "        config: settings for connect\n",
    "        sql: sql to read, it may be one of these format\n",
    "             - 'table_name'\n",
    "             - 'schema_name.table_name'\n",
    "             - '(select a, b, c from t1 join t2 ...) as foo'\n",
    "        spark: specific current spark_context or None\n",
    "    Returns:\n",
    "        selected DF\n",
    "    \"\"\"        \n",
    "    try:\n",
    "        print(\"reading from postgresql\")\n",
    "        if sql:\n",
    "            print(\"executing query to create df\")\n",
    "            source_df = spark.read.format(\"jdbc\").options(**config).option(\"query\", sql).load()\n",
    "\n",
    "        else:\n",
    "            print(\"reading directly from table\")\n",
    "            source_df = spark.read.format(\"jdbc\").options(**config).option('dbtable',table).load()\n",
    "\n",
    "        return source_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        print(f\"{e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f468c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_pg(spark:SparkSession, df: DataFrame, config: dict, table: str, mode: str='append' ) -> None:\n",
    "    \"\"\" Write dataframe to postgres\n",
    "    Args:\n",
    "        df: DataFrame to write\n",
    "        config: config dict\n",
    "        table: table_name in which we write_data\n",
    "        column_list: list of columns in which we write the data\n",
    "        mode: mode, one of these:\n",
    "            - append - create table if not exists (with all columns of DataFrame)\n",
    "                and write records to table (using fields only in table columns)\n",
    "            - overwrite - truncate table (if exists) and write records (using fields only in table columns)\n",
    "            - overwrite_full - drop table and create new one with all columns and DataFrame and append records to it\n",
    "            - fail - fail if table is not exists, otherwise append records to it\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        column_list = df.columns\n",
    "        if len(column_list) == 0:\n",
    "            return(\"No columns to write into\")\n",
    "\n",
    "        else:\n",
    "            df.select(*column_list).write.format('jdbc').options(**config).option('dbtable',table).mode(mode).save()\n",
    "            return \"Data written into postgresql successfully\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        print(f\"{e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca9a588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_file(spark:SparkSession, filename:str, delimiter:str) -> DataFrame:\n",
    "    \"\"\" Read dataframe from manual file\n",
    "    Args:\n",
    "        filename: file to read from\n",
    "        delimiter: delimiter to use when reading the file\n",
    "    Returns:\n",
    "        selected DF\n",
    "    \"\"\"       \n",
    "    try:\n",
    "        print(\"reading from file\")\n",
    "        source_df = spark.read.option(\"delimiter\", delimiter).option('inferschema',True).option('header',True).csv('sourcedata/'+filename)  \n",
    "        print(\"file read successfully, returning dataframe\")\n",
    "        return source_df \n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        print(f\"{e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0065c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(df:DataFrame, filename:str, delimiter:str = ',', mode: str='append' )->None:\n",
    "    \"\"\"Write dataframe to a csv file\n",
    "    Args:\n",
    "    df: data frame to write\n",
    "    filename: filename with which we want to save our dataframe in a file\n",
    "    by default we will write a comma delimited file only \n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"writing the dataframe\")\n",
    "        df.coalesce(1).write.option('delimiter', delimiter).option(\"header\", True).csv('target/'+filename, mode=mode)\n",
    "        print(\"dataframe written at target/\"+filename)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        print(f\"{e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c128bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_oracle(spark:SparkSession, config: dict, sql: str, table: str) -> DataFrame:\n",
    "    \"\"\" Read dataframe from oracle\n",
    "    Args:\n",
    "        config: settings for connect\n",
    "        sql: sql to read, it may be one of these format\n",
    "             - 'table_name'\n",
    "             - 'schema_name.table_name'\n",
    "             - '(select a, b, c from t1 join t2 ...) as foo'\n",
    "        spark: specific current spark_context or None\n",
    "    Returns:\n",
    "        selected DF\n",
    "    \"\"\"        \n",
    "    try:\n",
    "        print(\"reading from oracle\")\n",
    "        if sql:\n",
    "            print(\"executing query to create df\")\n",
    "            source_df = spark.read.format(\"jdbc\").options(**config).option(\"query\", sql).load()\n",
    "\n",
    "        else:\n",
    "            print(\"reading directly from table\")\n",
    "            source_df = spark.read.format(\"jdbc\").options(**config).option('dbtable',table).load()\n",
    "\n",
    "        return source_df \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        print(f\"{e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77896517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_oracle(spark:SparkSession, df: DataFrame, config: dict, table: str, mode: str='append' ) -> None:\n",
    "    \"\"\" Write dataframe to oracle\n",
    "    Args:\n",
    "        df: DataFrame to write\n",
    "        config: config dict\n",
    "        table: table_name in which we write_data\n",
    "        column_list: list of columns in which we write the data\n",
    "        mode: mode, one of these:\n",
    "            - append - create table if not exists (with all columns of DataFrame)\n",
    "                and write records to table (using fields only in table columns)\n",
    "            - overwrite - truncate table (if exists) and write records (using fields only in table columns)\n",
    "            - overwrite_full - drop table and create new one with all columns and DataFrame and append records to it\n",
    "            - fail - fail if table is not exists, otherwise append records to it\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        column_list = df.columns\n",
    "        if len(column_list) == 0:\n",
    "            return(\"No columns to write into\")\n",
    "\n",
    "        else:\n",
    "            df.select(*column_list).write.format('jdbc').options(**config).option('dbtable',table).mode(mode).save()\n",
    "            return \"Data written into oracle successfully\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        print(f\"{e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c637cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_msssql(spark:SparkSession, config: dict, sql: str, table: str) -> DataFrame:\n",
    "    \"\"\" Read dataframe from msssql\n",
    "    Args:\n",
    "        config: settings for connect\n",
    "        sql: sql to read, it may be one of these format\n",
    "             - 'table_name'\n",
    "             - 'schema_name.table_name'\n",
    "             - '(select a, b, c from t1 join t2 ...) as foo'\n",
    "        spark: specific current spark_context or None\n",
    "    Returns:\n",
    "        selected DF\n",
    "    \"\"\"       \n",
    "    try:\n",
    "        print(\"reading from msssql\")\n",
    "        if sql:\n",
    "            print(\"executing query to create df\")\n",
    "            source_df = spark.read.format(\"jdbc\").options(**config).option(\"query\", sql).load()\n",
    "\n",
    "        else:\n",
    "            print(\"reading directly from table\")\n",
    "            source_df = spark.read.format(\"jdbc\").options(**config).option('dbtable',table).load()\n",
    "\n",
    "        return source_df \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        print(f\"{e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aece80c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_mssql(spark:SparkSession, df: DataFrame, config: dict, table: str, mode: str='append' ) -> None:\n",
    "    \"\"\" Write dataframe to msssql\n",
    "    Args:\n",
    "        df: DataFrame to write\n",
    "        config: config dict\n",
    "        table: table_name in which we write_data\n",
    "        column_list: list of columns in which we write the data\n",
    "        mode: mode, one of these:\n",
    "            - append - create table if not exists (with all columns of DataFrame)\n",
    "                and write records to table (using fields only in table columns)\n",
    "            - overwrite - truncate table (if exists) and write records (using fields only in table columns)\n",
    "            - overwrite_full - drop table and create new one with all columns and DataFrame and append records to it\n",
    "            - fail - fail if table is not exists, otherwise append records to it\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        column_list = df.columns\n",
    "        if len(column_list) == 0:\n",
    "            return(\"No columns to write into\")\n",
    "\n",
    "        else:\n",
    "            df.select(*column_list).write.format('jdbc').options(**config).option('dbtable',table).mode(mode).save()\n",
    "            return \"Data written into msssql successfully\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        print(f\"{e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92ba5b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_target_dtypes(source_df:DataFrame, target_df:DataFrame, mappings:dict)-> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to convert source column names to target column names \n",
    "    and convert source column data types to target column data types\n",
    "    \n",
    "    Args:\n",
    "        source_df: source dataframe \n",
    "        target_df: target dataframe\n",
    "        mapping: dictionary with source to column mapping \n",
    "        date_format: date format to which we want to convert source date column to target date type\n",
    "        \n",
    "    returns :\n",
    "        returns a data frame which we can write in target\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        # date_format =  'dd-MM-yyyy'\n",
    "        \n",
    "        # mapping source columns to target\n",
    "        final_df = source_df\n",
    "        for key, value in mappings.items():\n",
    "            final_df= final_df.withColumnRenamed(key,value)\n",
    "            \n",
    "        target_types = target_df.dtypes\n",
    "        source_types = final_df.dtypes\n",
    "        # type conversion of source columns to target column types\n",
    "        for i in source_types:\n",
    "            for j in target_types:\n",
    "                if i[0] == j[0] and j[1]!='date' and j[1] !='timestamp':\n",
    "                    print(i[0])\n",
    "                    final_df = final_df.withColumn(i[0],final_df[i[0]].cast(j[1]))\n",
    "\n",
    "#                 elif i[0] == j[0] and j[1] =='date' :\n",
    "#                     print(i[0])\n",
    "#                     final_df = final_df.withColumn(i[0], to_timestamp(final_df[i[0]], source_date_format).cast('date') )\n",
    "                \n",
    "#                 elif i[0] == j[0] and j[1] =='timestamp' :\n",
    "#                     print(i[0])\n",
    "#                     final_df = final_df.withColumn(i[0], date_format(final_df[i[0]], target_date_format).cast('timestamp') )\n",
    "    \n",
    "        return final_df\n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        print(f\"{e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca554684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  mapping_generation(spark:SparkSession, mapping_filename):\n",
    "    \"\"\"\n",
    "    Function to read mapping file and do following:\n",
    "        generate source to column mapping\n",
    "        generate mapping of the date column to explicitly convert them\n",
    "        \n",
    "    Args:\n",
    "        spark: SparkSession object\n",
    "    Returns:\n",
    "         conversion_mapping_dict: dict\n",
    "             a dict column mapping for different use cases. \n",
    "             eg. : mapping source to target columns\n",
    "                   mapping columns for specific date format conversion\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mapping_file_path = 'mapping/'+mapping_filename\n",
    "        mapping_df = spark.read.option('header',True).option('inferSchema',True).csv(mapping_file_path)\n",
    "        # mapping_pdf = mapping_df.toPandas()\n",
    "        # mapping_dict = mapping_pdf.to_dict(orient='records')\n",
    "        mapping_dict = [row.asDict() for row in mapping_df.collect()]\n",
    "\n",
    "        source_target_col_mapping = {}\n",
    "        columns_for_date_conversion = []\n",
    "        static_target_columns = []\n",
    "        default_value_for_null_columns = []\n",
    "\n",
    "        for element in mapping_dict:\n",
    "            if (element['source'] != None):\n",
    "                source_target_col_mapping[element['source']] = element['target']\n",
    "\n",
    "            if (element['source_format']) and (element['target_format']):\n",
    "                columns_for_date_conversion.append(element)\n",
    "\n",
    "            if (element['source'] == None and element['target'] != None):\n",
    "                static_target_columns.append(element)\n",
    "\n",
    "            if (element['source'] != None and element['target'] != None and element['default_value'] != None ):\n",
    "                default_value_for_null_columns.append(element)\n",
    "\n",
    "        ## Need to add the code to coombine the result in a dic\n",
    "        conversion_mapping_dict = {}\n",
    "        conversion_mapping_dict['source_target_column_mapping'] = source_target_col_mapping\n",
    "        conversion_mapping_dict['columns_for_date_conversion'] = columns_for_date_conversion\n",
    "        conversion_mapping_dict['static_target_columns'] = static_target_columns\n",
    "        conversion_mapping_dict['default_value_for_null_columns'] = default_value_for_null_columns\n",
    "        return conversion_mapping_dict\n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        print(f\"{e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae80aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fill_na_dict(default_value_for_null_columns_list:list)->dict:\n",
    "    \"\"\"\n",
    "    Function to get the columns to fill with NA values\n",
    "    Args:\n",
    "        default_value_for_null_columns_list: list of the jsons contaning information about default values\n",
    "        for NA/NULLs\n",
    "    returns:\n",
    "        dict with column name to default value mapping\n",
    "    \"\"\"\n",
    "    fill_na_dict = {}\n",
    "    \n",
    "    for element in default_value_for_null_columns_list:\n",
    "        fill_na_dict[element['target']] = element['default_value']\n",
    "     \n",
    "    return fill_na_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9b34f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_null_values(df: DataFrame, fill_na_dict: dict)-> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to replace Nulls with hardcoded value proviede at mapping file\n",
    "    Args:\n",
    "        df: Spark dataframe in which data with null values is present\n",
    "        fill_na_dict: dictionary with mapping of column name and the value for replacing NULL\n",
    "    returns:\n",
    "        data frame with null values replaced with hard coded values provided by the mapping\n",
    "    \n",
    "    \"\"\"\n",
    "    if fill_na_dict:\n",
    "        res_df = df.fillna(fill_na_dict)\n",
    "        return res_df\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac98cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_column_with_default_values(df: DataFrame, static_target_columns_list:list)-> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to populate columns which are only present at target and need hardcoded/static value\n",
    "    Args:\n",
    "        df : spark dataframe inside which we will add the static values\n",
    "        static_target_columns_list : list of columns for which we need to add static columns \n",
    "        \n",
    "    returns: spark dataframe with target columns populated with hard coded values\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tmp = {} \n",
    "        for element in static_target_columns_list:  \n",
    "            df = df.withColumn(element['target'], lit(element['default_value']) )\n",
    "\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        print(f\"{e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9bae888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_column_format_converter(df:DataFrame,columns_for_date_conversion:list)-> DataFrame:\n",
    "    \"\"\"\n",
    "    Function to convert the date columns into requested format\n",
    "    Args:\n",
    "        Spark dataframe\n",
    "        columns_for_date_conversion: list of jsons contaning inforamtion about date columns \n",
    "    returns:\n",
    "        dataframe with date column modfied per the format given\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for column_details in columns_for_date_conversion:\n",
    "            source_format = column_details['source_format']\n",
    "            target_format = column_details['target_format']\n",
    "            target_col = column_details['target']\n",
    "            target_dtype = column_details['target_data_type']\n",
    "            print('target_col =', target_col,',source_format =',source_format,',target_format=', target_format,',target_dtype=',target_dtype) \n",
    "#             print(target_col)\n",
    "#             df = df.withColumn(target_col, to_timestamp(df[target_col].cast('string'), source_format))\n",
    "            tmp_df = df.withColumn(target_col, to_timestamp(col(target_col) , source_format)) \n",
    "    \n",
    "            if target_dtype == 'string':\n",
    "                print(\"target type is STRING\")\n",
    "                final_df = tmp_df.withColumn(target_col, date_format(to_timestamp(col(target_col), source_format), target_format))\n",
    "                \n",
    "            elif target_dtype == 'date':\n",
    "                final_df = tmp_df.withColumn(target_col, to_date(target_col, source_format))\n",
    "            elif target_dtype == 'timestamp':\n",
    "                final_df = tmp_df.withColumn(target_col, to_timestamp(target_col, source_format))\n",
    "\n",
    "        return final_df\n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        print(f\"{e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e54a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea69a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def driver_code(spark:SparkSession, source_info:dict, target_info:dict, mapping_filename:str):\n",
    "#     \"\"\"\n",
    "#     This is the driver code function\n",
    "#     the flow of execution based on type of source and target gets executed inside this function\n",
    "#     Args:\n",
    "#         Function takes two dicts as arugmnets which as source and target information\n",
    "#         a sample of these inputs is :\n",
    "#             source_config = {\n",
    "#                         \"source_name\" : \"\",\n",
    "#                         \"source_config\" : {\n",
    "#                                         \"url\" : \"\",\n",
    "#                                         \"driver\" : \"\",\n",
    "#                                         \"user\" : \"\",\n",
    "#                                         \"password\" : \"\"},\n",
    "#                         \"source_query\" : \"\",\n",
    "#                         \"source_table\" : \"\",\n",
    "#                         \"source_filename\" : \"\",\n",
    "#                         \"source_delimiter\" : \"\"\n",
    "#                     }\n",
    "\n",
    "#                     target_config = {\n",
    "\n",
    "#                         \"target_name\" : \"\",\n",
    "#                         \"target_config\" : {\n",
    "#                                         \"url\" : \"\",\n",
    "#                                         \"driver\" : \"\",\n",
    "#                                         \"user\" : \"\",\n",
    "#                                         \"password\" : \"\"\n",
    "#                                         },\n",
    "#                         \"target_table\" : \"\",\n",
    "#                         \"target_filename\" : \"\",\n",
    "#                         \"target_delimiter\" : \"\",\n",
    "#                         \"target_write_mode\" : \"\" \n",
    "#                     }\n",
    "#     \"\"\"\n",
    "#     source_name = source_info['source_name']\n",
    "#     source_config = source_info['source_config']\n",
    "#     source_query = source_info['source_query']\n",
    "#     source_table = source_info['source_table']\n",
    "#     source_filename = source_info['source_filename']\n",
    "#     source_delimiter = source_info['source_delimiter']\n",
    "    \n",
    "#     target_name = target_info['target_name']\n",
    "#     target_config = target_info['target_config'] \n",
    "#     target_table = target_info['target_table']\n",
    "#     target_filename = target_info['target_filename'] \n",
    "#     target_delimiter = target_info['target_delimiter']\n",
    "#     target_write_mode = target_info['target_write_mode']\n",
    "    \n",
    "#     try:\n",
    "#         # reading from source into source_df\n",
    "#         if source_name == 'oracle':\n",
    "#             source_df = dbu.read_from_oracle(spark , source_config, source_query, source_table)\n",
    "\n",
    "#         elif source_name == 'sqlserver':\n",
    "#             source_df = dbu.read_from_msssql(spark , source_config, source_query, source_table)\n",
    "\n",
    "#         elif source_name == 'postgres':\n",
    "#             source_df = dbu.read_from_pg(spark , source_config, source_query, source_table)\n",
    "\n",
    "#         elif source_name == 'manualfile':\n",
    "#             source_df = dbu.read_csv_file(spark , source_filename, source_delimiter)  \n",
    "            \n",
    "            \n",
    "#         print('Data read from source')\n",
    "        \n",
    "#         if target_name != 'manualfile':\n",
    "            \n",
    "#             #reading from target for target_df creation\n",
    "#             if target_name == 'oracle':\n",
    "#                 target_df = dbu.read_from_oracle(spark , target_config, '', target_table)\n",
    "\n",
    "#             elif target_name == 'sqlserver':\n",
    "#                 target_df = dbu.read_from_msssql(spark , target_config, '', target_table)\n",
    "\n",
    "#             elif target_name == 'postgres':\n",
    "#                 target_df = dbu.read_from_pg(spark , target_config, '', target_table)\n",
    "\n",
    "\n",
    "#             # generating mappings\n",
    "#             mappings = dbu.mapping_generation(spark, mapping_filename)\n",
    "#             columns_for_date_conversion = mappings['columns_for_date_conversion']\n",
    "#             source_to_target_mapping = mappings['source_target_column_mapping']\n",
    "#             static_target_columns = mappings['static_target_columns']\n",
    "#             default_value_for_null_columns = mappings['default_value_for_null_columns']\n",
    "\n",
    "#             # converting primary data types \n",
    "#             print(\"****converting preliminary data types****\")\n",
    "#             type_converted_df = dbu.convert_to_target_dtypes(source_df, target_df, source_to_target_mapping)\n",
    "\n",
    "#             #converting date column types and formats\n",
    "#             print(\"****converting date column types and formats****\")\n",
    "#             date_converted_df = dbu.date_column_format_converter(type_converted_df ,columns_for_date_conversion)\n",
    "\n",
    "#             #adding values for hard coded columns\n",
    "#             print(\"****adding values for hard coded columns****\")\n",
    "#             hard_coded_value_populated_df = dbu.populate_column_with_default_values(date_converted_df, static_target_columns)\n",
    "\n",
    "#             #adding default values for nulls\n",
    "#             print(\"****adding default values for nulls****\")\n",
    "#             fill_na_dict = dbu.create_fill_na_dict(default_value_for_null_columns)\n",
    "#             print(\"****fill_na_dict:\", fill_na_dict)\n",
    "#             null_populated_df = dbu.populate_null_values(hard_coded_value_populated_df, fill_na_dict)\n",
    "\n",
    "#             #write to target\n",
    "#             print(\"****Writing to target\") \n",
    "#             columns_list = list(set(target_df.columns).intersection(null_populated_df.columns)) \n",
    "#             final_df = null_populated_df.select(*columns_list)\n",
    "#             if target_name == 'oracle':\n",
    "#                 dbu.write_to_oracle(spark, final_df, target_config, target_table, target_write_mode) \n",
    "\n",
    "#             elif target_name == 'sqlserver':\n",
    "#                 dbu.write_to_mssql(spark, final_df, target_config, target_table, target_write_mode) \n",
    "\n",
    "#             elif target_name == 'postgres':\n",
    "#                 dbu.write_to_pg(spark, final_df, target_config, target_table, target_write_mode) \n",
    "\n",
    "#         else:\n",
    "#             ## Need to add function to convert output for delimited files\n",
    "#             dbu.write_to_csv(source_df, target_filename, target_delimiter)\n",
    "        \n",
    "#         # print('Data written to target successfully')\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(\"Failure occured check logs\")\n",
    "#         print(f\"{e}\")\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2dc3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
