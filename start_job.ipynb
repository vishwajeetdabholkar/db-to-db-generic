{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9db1ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from logging import Logger\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3365f0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from db_utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import db_utils as dbu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2038b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_CONFIG = {\n",
    "    \"MASTER\": \"local[*]\",\n",
    "    \"settings\": {\n",
    "      \"spark.executor.cores\": \"1\",\n",
    "      \"spark.executor.memory\": \"1g\",\n",
    "      \"spark.driver.cores\": \"1\",\n",
    "      \"spark.driver.memory\": \"1g\",\n",
    "      \"spark.cores.max\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76f3a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark_context() -> SparkContext:\n",
    "    \"\"\" init spark context \"\"\"\n",
    "\n",
    "    # os.environ['PYSPARK_SUBMIT_ARGS'] = f'--jars jars/postgresql-42.5.0 pyspark-shell'\n",
    "    conf = SparkConf()\n",
    "    conf.setMaster(SPARK_CONFIG['MASTER'])\n",
    "    conf.setAppName('app')\n",
    "\n",
    "    for setting, value in SPARK_CONFIG['settings'].items():\n",
    "        conf.set(setting, value)\n",
    "\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "    return sc\n",
    "\n",
    "sc = init_spark_context()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "702c765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.conf import SparkConf\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark.sparkContext._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8786f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_config(source_name: str):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        source_name: source name for which we want to get the configuration details\n",
    "    Returns:\n",
    "        configuration dictinory with required details to read from source\n",
    "    \"\"\"\n",
    "    ## Path is hardcoded for devlopment need to change\n",
    "    source_file = r'C:\\Users\\vishwajeet.dabholkar\\Documents\\data-mapping\\migration\\config\\source_config.json'\n",
    "    with open(source_file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    return data[source_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f86221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_config(target_name: str):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        target_name: source name for which we want to get the configuration details\n",
    "    Returns:\n",
    "        configuration dictinory with required details to read from source\n",
    "    \"\"\"\n",
    "    ## Path is hardcoded for devlopment need to change\n",
    "    target_file = r'C:\\Users\\vishwajeet.dabholkar\\Documents\\data-mapping\\migration\\config\\target_config.json'\n",
    "    with open(target_file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    return data[target_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a931a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mappings(spark:SparkSession):\n",
    "    mapping_file_path = 'mapping/mapping.csv'\n",
    "    mapping_df = spark.read.option('header',True).csv(mapping_file_path)\n",
    "    source_column_list = list(mapping_df.select('source').toPandas()['source'])\n",
    "    target_column_list = list(mapping_df.select('target').toPandas()['target'])\n",
    "    source_target_dict = dict(zip(source_column_list, target_column_list))\n",
    "    return source_target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64704dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_query(filename:str)-> str:\n",
    "    \"\"\"\n",
    "    Function to read queyr file and return it as string\n",
    "    args:\n",
    "        filename : Name of the file in which query is stored. '.sq' format is recommended\n",
    "    \"\"\"\n",
    "    if os.path.isfile('query/'+filename):\n",
    "        text_file = open('query/'+filename, \"r\")\n",
    " \n",
    "        #read whole file to a string\n",
    "        data = text_file.read()\n",
    "\n",
    "        #close file\n",
    "        text_file.close()\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        return 'File not present'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a220ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_input_config():\n",
    "    \n",
    "    sources= {1:\"manualfile\",\n",
    "             2:\"oracle\",\n",
    "             3:\"sqlserver\",\n",
    "             4:\"postgresql\"\n",
    "            }\n",
    "    \n",
    "    print(\"Select Source:\")\n",
    "    print(\"1.Manual File\\n2.Oracle\\n3.MSSQL\\n4.PostgreSQL\")\n",
    "    inputs_source_name =  int(input('source_name: '))\n",
    "    \n",
    "    source_name = ''\n",
    "    for k,v in sources.items():\n",
    "        if inputs_source_name == k:\n",
    "            source_name = v\n",
    "    source_config = get_source_config(source_name)\n",
    "    \n",
    "    if source_name == 'manualfile':\n",
    "        file_path = input('file path: ')\n",
    "        delimeter = input('delimeter: ')\n",
    "        source_config['filepath'] = file_path\n",
    "        source_config['delimeter'] = delimeter\n",
    "        \n",
    "    else:\n",
    "        url = input('jdbc url : ')\n",
    "        user = input('user : ')\n",
    "        password = input('password : ')\n",
    "        dbtable = input('database to read from : ')\n",
    "        source_config['config']['url'] = url\n",
    "        source_config['config']['user'] = user\n",
    "        source_config['config']['password'] = password\n",
    "        \n",
    "        if dbtable == '':\n",
    "            query_path = input('query path: ')\n",
    "            query = read_query(query_path)\n",
    "            source_config['config']['query'] = query\n",
    "        else:\n",
    "            source_config['config']['dbtable'] = dbtable\n",
    "\n",
    "    return source_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_input_config():\n",
    "    \n",
    "    targets= {1:\"manualfile\",\n",
    "             2:\"oracle\",\n",
    "             3:\"sqlserver\",\n",
    "             4:\"postgresql\"\n",
    "            }\n",
    "    \n",
    "    print(\"Select Source:\")\n",
    "    print(\"1.Manual File\\n2.Oracle\\n3.MSSQL\\n4.PostgreSQL\")\n",
    "    inputs_target_name =  int(input('source_name: '))\n",
    "    \n",
    "    target_name = ''\n",
    "    for k,v in targets.items():\n",
    "        if inputs_target_name == k:\n",
    "            target_name = v\n",
    "    target_config = get_target_config(target_name)\n",
    "    \n",
    "    if target_name == 'manualfile':\n",
    "        file_path = input('file path: ')\n",
    "        delimeter = input('delimeter: ')\n",
    "        \n",
    "        target_config['filepath'] = file_path\n",
    "        target_config['delimeter'] = delimeter\n",
    "        \n",
    "    else:\n",
    "        url = input('jdbc url : ')\n",
    "        user = input('user : ')\n",
    "        password = input('password : ')\n",
    "        dbtable = input('database table to write into : ')\n",
    "        target_config['config']['url'] = url\n",
    "        target_config['config']['user'] = user\n",
    "        target_config['config']['password'] = password\n",
    "        target_config['config']['dbtable'] = dbtable\n",
    "\n",
    "    return target_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d1e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_to_tarrget_df_conversion(source_df: DataFrame, mapping_dict: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Converting source dataframe column names to target column names\n",
    "    Args:\n",
    "        source_df: dataframe which we convert for target\n",
    "        mapping_dict: dictornary with mapping for source_col to target_col\n",
    "    \"\"\"\n",
    "    final_df = source_df\n",
    "    for key, value in mapping_dict.items():\n",
    "        final_df= final_df.withColumnRenamed(key,value)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866cfde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbff0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Logins for Informatica:\n",
    "# CMX_ORS:\n",
    "# Username: cmx_ors\n",
    "# Password: cmx_ors\n",
    "# CMX_System(metadata):\n",
    "# Username: cmx_system\n",
    "# Password: cmx_system\n",
    "# Assigned IP:- 192.168.2.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3019c02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "# \"url\" : \"jdbc:oracle:thin:@192.168.2.70:1521/PDB\",\n",
    "# \"driver\" : \"oracle.jdbc.driver.OracleDriver\",\n",
    "# \"user\" : \"cmx_ors\",\n",
    "# \"password\" : \"cmx_ors\",\n",
    "# \"dbtable\" : \"C_BO_PTY\"}\n",
    "\n",
    "# df = spark.read.format(\"jdbc\").options(**config).load()\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203f755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca03419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b5cb55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
