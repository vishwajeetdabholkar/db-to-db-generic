{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9db1ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf \n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql import DataFrame\n",
    "from logging import Logger\n",
    "import os\n",
    "import json\n",
    "import psycopg2\n",
    "import pyodbc \n",
    "import cx_Oracle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3365f0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from db_utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import db_utils as dbu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2038b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_CONFIG = {\n",
    "    \"MASTER\": \"local[*]\",\n",
    "    \"settings\": {\n",
    "      \"spark.executor.cores\": \"1\",\n",
    "      \"spark.executor.memory\": \"1g\",\n",
    "      \"spark.driver.cores\": \"1\",\n",
    "      \"spark.driver.memory\": \"1g\",\n",
    "      \"spark.cores.max\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76f3a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark_context() -> SparkContext:\n",
    "    \"\"\" init spark context \"\"\"\n",
    "\n",
    "    # os.environ['PYSPARK_SUBMIT_ARGS'] = f'--jars jars/postgresql-42.5.0 pyspark-shell'\n",
    "    conf = SparkConf()\n",
    "    conf.setMaster(SPARK_CONFIG['MASTER'])\n",
    "    conf.setAppName('app')\n",
    "\n",
    "    for setting, value in SPARK_CONFIG['settings'].items():\n",
    "        conf.set(setting, value)\n",
    "\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "    return sc\n",
    "\n",
    "sc = init_spark_context()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702c765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.conf import SparkConf\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark.sparkContext._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8786f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a31a9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_target_details(pid:int):\n",
    "    \"\"\"\n",
    "    Function to get source and target details from backedn metadeta table and convert them\n",
    "    into a dict format which is required by driver code\n",
    "    Args:\n",
    "        pid: process id or pid to query the details from meta deta table\n",
    "    returns:\n",
    "        source_details, target_details dicts for used in driver code\n",
    "        and mapping file, name of mapping file stored in the mappings folder\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # create connection to metadata table\n",
    "        host = '192.168.2.26'\n",
    "        database = 'postgres'\n",
    "        user = 'postgres'\n",
    "        password = 'Fresh*123'\n",
    "        conn = psycopg2.connect(\n",
    "                        host=host,\n",
    "                        database=database,\n",
    "                        user=user,\n",
    "                        password=password)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # queries to get source and target details for metadeta table\n",
    "        # process id or pid should be give as parameter while invoking this function = 4\n",
    "        source_details_query = f'''SELECT  \n",
    "        \"SRC_INPUT_TYPE\", \n",
    "        \"SRC_QUERY\", \n",
    "        \"SRC_TABLE\", \n",
    "        \"SRC_CONNECTION_NAME\",\n",
    "        \"SRC_CONNECTION_TYPE\",\n",
    "        \"SRC_DATABASE_DRIVER\",\n",
    "        (case when (\"SRC_DATABASE_NAME\" IS NOT NULL) then \"SRC_DATABASE_NAME\"\n",
    "        else 'delimited_file' end ) as \"SRC_DATABASE_NAME\",\n",
    "        \"SRC_JDBCURL\",\n",
    "        \"SRC_USER_NAME\", \n",
    "        \"SRC_PASSWORD\",\n",
    "        \"SRC_FILE_PATH\", \n",
    "        \"SRC_FILE_NAME\", \n",
    "        \"SRC_DELIMITER\"\n",
    "\n",
    "        FROM semarchy_data_migration_utility.dbv_get_process_vw\n",
    "        where \"PROCESS_ID\" = {pid}; \n",
    "        '''\n",
    "\n",
    "        target_details_query = f'''SELECT  \n",
    "        \"TRG_TABLE\",\n",
    "        \"TRG_CONNECTION_NAME\", \n",
    "        \"TRG_CONNECTION_TYPE\", \n",
    "        \"TRG_DATABASE_DRIVER\", \n",
    "        (case when (\"TRG_DATABASE_NAME\" IS NOT NULL) then \"TRG_DATABASE_NAME\"\n",
    "        else 'delimited_file' end ) as \"TRG_DATABASE_NAME\", \n",
    "        \"TRG_JDBCURL\", \n",
    "        \"TRG_USER_NAME\", \n",
    "        \"TRG_PASSWORD\", \n",
    "        \"TRG_FILE_PATH\", \n",
    "        \"TRG_FILE_NAME\", \n",
    "        \"TRG_DELIMITER\", \n",
    "        \"MAPPING_NAME\",\n",
    "        \"TRG_WRITE_MODE\"\n",
    "        FROM semarchy_data_migration_utility.dbv_get_process_vw\n",
    "        where \"PROCESS_ID\" = {pid}; \n",
    "        '''\n",
    "\n",
    "        # read the queries in pandas df\n",
    "        source_details_df = pd.read_sql(source_details_query, conn)\n",
    "        target_details_df = pd.read_sql(target_details_query, conn)\n",
    "\n",
    "        # create dictionary out of pandas dfs\n",
    "        source_dict = source_details_df.to_dict()\n",
    "        source_dict2 = {key:value[0] for key,value in source_dict.items() }\n",
    "\n",
    "        target_dict = target_details_df.to_dict()\n",
    "        target_dict2 = {key:value[0] for key,value in target_dict.items() }\n",
    "\n",
    "        # source and target config templated to be passed for driver code \n",
    "        tmp_src = {\n",
    "        \"source_name\" : \"SRC_DATABASE_NAME\",\n",
    "        \"source_config\" : {\n",
    "                        \"url\" : \"SRC_JDBCURL\",\n",
    "                        \"driver\" : \"SRC_DATABASE_DRIVER\",\n",
    "                        \"user\" : \"SRC_USER_NAME\",\n",
    "                        \"password\" : \"SRC_PASSWORD\"},\n",
    "        \"source_query\" : \"SRC_QUERY\",\n",
    "        \"source_table\" : \"SRC_TABLE\",\n",
    "        \"source_filename\" : \"SRC_FILE_NAME\",\n",
    "        \"source_delimiter\" : \"SRC_DELIMITER\"\n",
    "        }\n",
    "\n",
    "        tmp_trg = {\n",
    "            \"target_name\" : \"TRG_DATABASE_NAME\",\n",
    "            \"target_config\" : {\n",
    "                            \"url\" : \"TRG_JDBCURL\" ,\n",
    "                            \"driver\" : \"TRG_DATABASE_DRIVER\",\n",
    "                            \"user\" : \"TRG_USER_NAME\",\n",
    "                            \"password\" : \"TRG_PASSWORD\"\n",
    "                            },\n",
    "            \"target_table\" : \"TRG_TABLE\",\n",
    "            \"target_filename\" : \"TRG_FILE_NAME\",\n",
    "            \"target_delimiter\" : \"TRG_DELIMITER\",\n",
    "            \"target_write_mode\" : \"TRG_WRITE_MODE\" \n",
    "        }\n",
    "\n",
    "        # logic to convert the dicts in required format \n",
    "        source_details = {}\n",
    "        source_config = {}\n",
    "        for key,value in source_dict2.items():\n",
    "            for k,v in tmp_src['source_config'].items():\n",
    "                if key == v:\n",
    "                    source_config[k] = value\n",
    "\n",
    "        for key,value in source_dict2.items():\n",
    "            source_details['source_config'] = source_config\n",
    "            for k,v in tmp_src.items():\n",
    "                if key == v:\n",
    "                    source_details[k] = value\n",
    "\n",
    "        print('source_details == ', source_details )\n",
    "\n",
    "\n",
    "        target_details = {}\n",
    "        target_config = {}\n",
    "        for key,value in target_dict2.items():\n",
    "            for k,v in tmp_trg['target_config'].items():\n",
    "                if key == v:\n",
    "                    target_config[k] = value\n",
    "\n",
    "        for key,value in target_dict2.items():\n",
    "            target_details['target_config'] = target_config\n",
    "            for k,v in tmp_trg.items():\n",
    "                if key == v:\n",
    "                    target_details[k] = value\n",
    "\n",
    "        print('target_details == ', target_details )\n",
    "        mapping_fileName = target_dict2['MAPPING_NAME']\n",
    "        print('Mapping_file Name = ',mapping_fileName)\n",
    "        return source_details, target_details, mapping_fileName\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        print(f\"{e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eb27b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "723787a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver_code(spark:SparkSession, source_info:dict, target_info:dict, mapping_filename:str):\n",
    "    \"\"\"\n",
    "    This is the driver code function\n",
    "    the flow of execution based on type of source and target gets executed inside this function\n",
    "    Args:\n",
    "        Function takes two dicts as arugmnets which as source and target information\n",
    "        a sample of these inputs is :\n",
    "            source_config = {\n",
    "                        \"source_name\" : \"\",\n",
    "                        \"source_config\" : {\n",
    "                                        \"url\" : \"\",\n",
    "                                        \"driver\" : \"\",\n",
    "                                        \"user\" : \"\",\n",
    "                                        \"password\" : \"\"},\n",
    "                        \"source_query\" : \"\",\n",
    "                        \"source_table\" : \"\",\n",
    "                        \"source_filename\" : \"\",\n",
    "                        \"source_delimiter\" : \"\"\n",
    "                    }\n",
    "\n",
    "                    target_config = {\n",
    "\n",
    "                        \"target_name\" : \"\",\n",
    "                        \"target_config\" : {\n",
    "                                        \"url\" : \"\",\n",
    "                                        \"driver\" : \"\",\n",
    "                                        \"user\" : \"\",\n",
    "                                        \"password\" : \"\"\n",
    "                                        },\n",
    "                        \"target_table\" : \"\",\n",
    "                        \"target_filename\" : \"\",\n",
    "                        \"target_delimiter\" : \"\",\n",
    "                        \"target_write_mode\" : \"\" \n",
    "                    }\n",
    "    \"\"\"\n",
    "    source_name = source_info['source_name'].lower()\n",
    "    source_config = source_info['source_config']\n",
    "    source_query = source_info['source_query']\n",
    "    source_table = source_info['source_table']\n",
    "    source_filename = source_info['source_filename']\n",
    "    source_delimiter = source_info['source_delimiter']\n",
    "    \n",
    "    target_name = target_info['target_name'].lower()\n",
    "    target_config = target_info['target_config'] \n",
    "    target_table = target_info['target_table']\n",
    "    target_filename = target_info['target_filename'] \n",
    "    target_delimiter = target_info['target_delimiter']\n",
    "    target_write_mode = target_info['target_write_mode'].lower()\n",
    "    \n",
    "    try:\n",
    "        # reading from source into source_df\n",
    "        if source_name == 'oracle':\n",
    "            source_df = dbu.read_from_oracle(spark , source_config, source_query, source_table)\n",
    "\n",
    "        elif source_name == 'sqlserver':\n",
    "            source_df = dbu.read_from_msssql(spark , source_config, source_query, source_table)\n",
    "\n",
    "        elif source_name == 'postgres':\n",
    "            source_df = dbu.read_from_pg(spark , source_config, source_query, source_table)\n",
    "\n",
    "        elif source_name == 'delimited_file':\n",
    "            source_df = dbu.read_csv_file(spark , source_filename, source_delimiter)  \n",
    "            \n",
    "            \n",
    "        print('Data read from source')\n",
    "        \n",
    "        if target_name != 'delimited_file':\n",
    "            \n",
    "            #reading from target for target_df creation\n",
    "            if target_name == 'oracle':\n",
    "                target_df = dbu.read_from_oracle(spark , target_config, '', target_table)\n",
    "\n",
    "            elif target_name == 'sqlserver':\n",
    "                target_df = dbu.read_from_msssql(spark , target_config, '', target_table)\n",
    "\n",
    "            elif target_name == 'postgres':\n",
    "                target_df = dbu.read_from_pg(spark , target_config, '', target_table)\n",
    "\n",
    "\n",
    "            # generating mappings\n",
    "            mappings = dbu.mapping_generation(spark, mapping_filename)\n",
    "            columns_for_date_conversion = mappings['columns_for_date_conversion']\n",
    "            source_to_target_mapping = mappings['source_target_column_mapping']\n",
    "            static_target_columns = mappings['static_target_columns']\n",
    "            default_value_for_null_columns = mappings['default_value_for_null_columns']\n",
    "\n",
    "            # converting primary data types \n",
    "            print(\"****converting preliminary data types****\")\n",
    "            print(\"columns converted: \")\n",
    "            type_converted_df = dbu.convert_to_target_dtypes(source_df, target_df, source_to_target_mapping)\n",
    "\n",
    "            #converting date column types and formats\n",
    "            print(\"****converting date column types and formats****\")\n",
    "            # print(columns_for_date_conversion)\n",
    "            date_converted_df = dbu.date_column_format_converter(type_converted_df ,columns_for_date_conversion)\n",
    "\n",
    "            #adding values for hard coded columns\n",
    "            print(\"****adding values for hard coded columns****\")\n",
    "            print(\"columns converted: \")\n",
    "            hard_coded_value_populated_df = dbu.populate_column_with_default_values(date_converted_df, static_target_columns)\n",
    "\n",
    "            #adding default values for nulls\n",
    "            print(\"****adding default values for nulls****\")\n",
    "            fill_na_dict = dbu.create_fill_na_dict(default_value_for_null_columns)\n",
    "            fill_na_dict = {k.lower(): v for k, v in fill_na_dict.items()}\n",
    "            print(\"\\tcolumns with default values for NULL:\", fill_na_dict)\n",
    "            null_populated_df = dbu.populate_null_values(hard_coded_value_populated_df, fill_na_dict)\n",
    "\n",
    "            #write to target\n",
    "            print(\"****Writing to target\") \n",
    "            columns_list = list(set(target_df.columns).intersection(null_populated_df.columns)) \n",
    "            final_df = null_populated_df.select(*columns_list)\n",
    "            if target_name == 'oracle':\n",
    "                dbu.write_to_oracle(spark, final_df, target_config, target_table, target_write_mode) \n",
    "\n",
    "            elif target_name == 'sqlserver':\n",
    "                dbu.write_to_mssql(spark, final_df, target_config, target_table, target_write_mode) \n",
    "\n",
    "            elif target_name == 'postgres':\n",
    "                dbu.write_to_pg(spark, final_df, target_config, target_table, target_write_mode) \n",
    "\n",
    "        else:\n",
    "            ## Need to add function to convert output for delimited_files\n",
    "            dbu.write_to_csv(source_df, target_filename, target_delimiter, target_write_mode)\n",
    "        \n",
    "        print('Process completed')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Failure occured check logs\")\n",
    "        print(f\"{e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard coded details for testing the code\n",
    "source_details = {\n",
    "            \"source_name\" : \"oracle\",\n",
    "            \"source_config\" : {\n",
    "                            \"url\" : \"jdbc:oracle:thin:@192.168.2.70:1521/PDB\",\n",
    "                            \"driver\" : \"oracle.jdbc.driver.OracleDriver\",\n",
    "                            \"user\" : \"cmx_ors\",\n",
    "                            \"password\" : \"cmx_ors\"},\n",
    "            \"source_query\" : \"\",\n",
    "            \"source_table\" : \"C_BO_ADDR\",\n",
    "            \"source_filename\" : \"\",\n",
    "            \"source_delimiter\" : \"\"\n",
    "        }\n",
    "\n",
    "target_details = {\n",
    "\n",
    "    \"target_name\" : \"postgres\",\n",
    "    \"target_config\" : {\n",
    "                    \"url\" : \"jdbc:postgresql://localhost:5432/postgres\",\n",
    "                    \"driver\" : \"org.postgresql.Driver\",\n",
    "                    \"user\" : \"postgres\",\n",
    "                    \"password\" : \"admin\"\n",
    "                    },\n",
    "    \"target_table\" : \"public.source_address\",\n",
    "    \"target_filename\" : \"\",\n",
    "    \"target_delimiter\" : \"\",\n",
    "    \"target_write_mode\" : \"append\" \n",
    "}\n",
    "\n",
    "mapping_filename = 'query_mapping.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ae5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "1,4,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36c89319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishwajeet.dabholkar\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\vishwajeet.dabholkar\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_details ==  {'source_config': {'driver': 'oracle.jdbc.OracleDriver', 'url': 'jdbc:oracle:thin:@192.168.2.70:1521/PDB', 'user': 'mt_ors', 'password': 'mt_ors'}, 'source_query': None, 'source_table': 'C_BO_COMN_CHANNEL_PREF', 'source_name': 'oracle', 'source_filename': None, 'source_delimiter': None}\n",
      "target_details ==  {'target_config': {'driver': 'org.postgresql.Driver', 'url': 'jdbc:postgresql://192.168.2.26:5432/postgres', 'user': 'postgres', 'password': 'Fresh*123'}, 'target_table': 'semarchy_fg_customer_b2c_stg.comm_chan_pref_stg', 'target_name': 'postgres', 'target_filename': None, 'target_delimiter': None, 'target_write_mode': 'Append'}\n",
      "Mapping_file Name =  Comm Channel.csv\n"
     ]
    }
   ],
   "source": [
    "pid = 3\n",
    "source_info,target_info,mapping_filename = get_source_target_details(pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b95e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_info['source_query'] = \"\"\"SELECT a.* ,\n",
    "STREET,  CITY,  STATE,  POSTAL_CD,  COUNTRY,  NRMLSD_STREET,  NRMLSD_CITY,  NRMLSD_STATE\n",
    "\n",
    "FROM MT_ORS.C_BO_ADDRESS b\n",
    "JOIN (\n",
    "SELECT \n",
    "ROWID_OBJECT,  FIRST_NAME,  LAST_NAME,  PHN_FIRST_NAME,  PHN_LAST_NAME,  NICK_NAME,  MEMBER_ID,  DATE_OF_BIRTH,  SRC_EMAIL,  VALUE_STS,  PERSON_TYP,  SRC_PHONE  FROM MT_ORS.C_BO_INDIVIDUAL ) a\n",
    "ON a.ROWID_OBJECT  = b.ROWID_OBJECT \n",
    "\"\"\"\n",
    "source_info['source_table'] = ''\n",
    "mapping_filename = 'query_mapping.csv'\n",
    "source_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feaa7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "# driver_code(spark, source_details, target_details, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e23a240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from oracle\n",
      "reading directly from table\n",
      "Data read from source\n",
      "reading from postgresql\n",
      "reading directly from table\n",
      "****converting preliminary data types****\n",
      "columns converted: \n",
      "communication_type\n",
      "channel_frequency\n",
      "preference\n",
      "first_name\n",
      "****converting date column types and formats****\n",
      "****adding values for hard coded columns****\n",
      "columns converted: \n",
      "****adding default values for nulls****\n",
      "\tcolumns with default values for NULL: {}\n",
      "****Writing to target\n",
      "Process completed\n"
     ]
    }
   ],
   "source": [
    "# for real time\n",
    "driver_code(spark, source_info, target_info, mapping_filename)\n",
    "# source_df = spark.read.format(\"jdbc\").options(**source_config['source_config']).option(\"query\", source_config['source_query']).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866cfde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea2342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc62d0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6d668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac6a315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
